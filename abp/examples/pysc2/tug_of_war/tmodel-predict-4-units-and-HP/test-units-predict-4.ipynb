{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm\n",
    "import os\n",
    "import uuid\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "IntTensor = torch.cuda.IntTensor if use_cuda else torch.IntTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "unique_id = str(uuid.uuid4())\n",
    "\n",
    "def weights_initialize(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        module.bias.data.fill_(0.01)\n",
    "        \n",
    "class _TransModel(nn.Module):\n",
    "    \"\"\" Model for DQN \"\"\"\n",
    "\n",
    "    def __init__(self, input_len, output_len):\n",
    "        super(_TransModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            torch.nn.Linear(input_len, 1024),\n",
    "            #torch.nn.BatchNorm1d(1024),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.fc1.apply(weights_initialize)\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            torch.nn.Linear(1024, 256),\n",
    "            # torch.nn.BatchNorm1d(128),\n",
    "            nn.Softplus()\n",
    "        )\n",
    "        self.fc2.apply(weights_initialize)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(256, output_len)\n",
    "        )\n",
    "        self.output_layer.apply(weights_initialize)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "class TransModel():\n",
    "    def __init__(self, input_len, ouput_len, learning_rate = 0.0001):\n",
    "        self.model = _TransModel(input_len, ouput_len)\n",
    "        \n",
    "        if use_cuda:\n",
    "            print(\"Using GPU\")\n",
    "            self.model = self.model.cuda()\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "        self.steps = 0\n",
    "        # self.model = nn.DataParallel(self.model)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr = learning_rate)\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        self.steps = 0\n",
    "        \n",
    "    def predict(self, input, steps, learning):\n",
    "        \n",
    "        output = self.model(input).squeeze(1)\n",
    "        #reward, next_state = output[0], output[1:]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict_batch(self, input):\n",
    "        output = self.model(input)\n",
    "        #reward, next_state = output[:, 0], output[:, 1:]\n",
    "        return output\n",
    "\n",
    "    def fit(self, state, target_state):\n",
    "        loss = self.loss_fn(state, target_state)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps += 1\n",
    "        return loss\n",
    "    \n",
    "    def save(self):\n",
    "        file_path = 'units-transition-model-predict-4.pt'\n",
    "        torch.save(self.model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dot_plot(x, y, fig, title):\n",
    "    print(len(x), len(y))\n",
    "    legend = [\n",
    "                \"Match line\",\n",
    "                \"Player 1 Top Marines\",\n",
    "                \"Player 1 Top Banelings\",\n",
    "                \"Player 1 Top Immortals\",\n",
    "                \"Player 1 Bottom Marines\",\n",
    "                \"Player 1 Bottom Banelings\",\n",
    "                \"Player 1 Bottom Immortals\",\n",
    "                \"Player 2 Top Marines\",\n",
    "                \"Player 2 Top Banelings\",\n",
    "                \"Player 2 Top Immortals\",\n",
    "                \"Player 2 Bottom Marines\",\n",
    "                \"Player 2 Bottom Banelings\",\n",
    "                \"Player 2 Bottom Immortals\",\n",
    "             ]\n",
    "    fig=plt.figure(figsize=(15, 15), dpi= 160, facecolor='w', edgecolor='k')\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "    ax.set_ylim([-10,140])\n",
    "    ax.set_xlim([-10,140])\n",
    "    ax.plot(list(range(-10, 140)),list(range(-10,140)), \"b--\", alpha=0.05)\n",
    "    for i in range(0, len(legend) - 1):\n",
    "        ax.scatter(x[:, i].view(-1).tolist(), y[:, i].view(-1).tolist(),s = 0.5)\n",
    "\n",
    "    plt.title(str(title))\n",
    "    plt.legend(legend, bbox_to_anchor=(0, 1), loc='upper left', ncol=1)\n",
    "    plt.xlabel(\"Ground Truth\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    chartBox = ax.get_position()\n",
    "    ax.set_position([chartBox.x0, chartBox.y0, chartBox.width, chartBox.height])\n",
    "    ax.legend(legend, loc='center left', bbox_to_anchor=(1, 0.8), shadow=True, ncol=1)\n",
    "\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, data, epoch):\n",
    "    state_action = torch.from_numpy(np.stack(data[:, 0])).type(FloatTensor)\n",
    "    next_state_reward = torch.from_numpy(np.stack(data[:, 1])).type(FloatTensor)\n",
    "    \n",
    "    model.model.eval()\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "    outputs = model.predict_batch(state_action)\n",
    "\n",
    "    mse = criterion(outputs, next_state_reward)\n",
    "\n",
    "    accuracy = torch.sum( torch.sum( torch.eq( outputs, next_state_reward ) )).item()\n",
    "    accuracy = accuracy / (2 * outputs.size()[0])\n",
    "\n",
    "    if epoch % 1000 == 0:\n",
    "        grid_1 = dot_plot(next_state_reward[:, 0:12], outputs[:, 0:12], plt.figure(), 'Grid 1')\n",
    "        grid_1.show()\n",
    "        grid_2 = dot_plot(next_state_reward[:, 12:24], outputs[:, 12:24], plt.figure(), 'Grid 2')\n",
    "        grid_2.show()\n",
    "        grid_3 = dot_plot(next_state_reward[:, 24:36], outputs[:, 24:36], plt.figure(), 'Grid 3')\n",
    "        grid_3.show()\n",
    "        grid_4 = dot_plot(next_state_reward[:, 36:48], outputs[:, 36:48], plt.figure(), 'Grid 4')\n",
    "        grid_4.show()\n",
    "    model.model.train()\n",
    "    \n",
    "    #summary_test.add_scalar(\"MSE\", float(mse.item()), epoch)\n",
    "    return mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.load('train_dataset.pt')\n",
    "np.set_printoptions(suppress=True)\n",
    "l = len(data)\n",
    "for i in range(0, len(data)):\n",
    "    data[i][1] = np.concatenate(\n",
    "                (\n",
    "                data[i][1][15:18], data[i][1][18:21], data[i][1][21:24], data[i][1][24:27], # P1 Top Units\n",
    "                data[i][1][27:30], data[i][1][30:33], data[i][1][33:36], data[i][1][36:39], # P1 Bottom Units\n",
    "                data[i][1][39:42], data[i][1][42:45], data[i][1][45:48], data[i][1][48:51], # P2 Top Units\n",
    "                data[i][1][51:54], data[i][1][54:57], data[i][1][57:60], data[i][1][60:63], # P2 Bottom Units\n",
    "                ), axis = 0)\n",
    "    \n",
    "    data[i][0][1:4] = np.true_divide( data[i][0][1:4], 30) # Normalize P1 top buildings\n",
    "    data[i][0][4:7] = np.true_divide( data[i][0][4:7], 30) # Normalize P1 bottom buildings\n",
    "\n",
    "    data[i][0][8:11] = np.true_divide( data[i][0][8:11], 30) # Normalize P2 top buildings\n",
    "    data[i][0][11:14] = np.true_divide( data[i][0][11:14], 30) # Normalize P2 bottom buildings\n",
    "    \n",
    "    data[i][0][63] = data[i][0][63] / 2000 # Normalize P1 Top Nexus HP\n",
    "    data[i][0][64] = data[i][0][64] / 2000 # Normalize P2 Top Nexus HP\n",
    "    \n",
    "    data[i][0][65] = data[i][0][65] / 2000 # Normalize P1 Bottom Nexus HP\n",
    "    data[i][0][66] = data[i][0][66] / 2000 # Normalize P2 Bottom Nexus HP\n",
    "\n",
    "    data[i][0][0] = data[i][0][10] / 1500 # Normalize P1 Minerals\n",
    "    data[i][0] = np.append(data[i][0], np.concatenate((\n",
    "        data[i][0][15:18], data[i][0][18:21], data[i][0][21:24], data[i][0][24:27], # P1 Top Units\n",
    "        data[i][0][27:30], data[i][0][30:33], data[i][0][33:36], data[i][0][36:39], # P1 Bottom Units\n",
    "        data[i][0][39:42], data[i][0][42:45], data[i][0][45:48], data[i][0][48:51], # P2 Top Units\n",
    "        data[i][0][51:54], data[i][0][54:57], data[i][0][57:60], data[i][0][60:63], # P2 Bottom Units\n",
    "    ), axis=0))\n",
    "\n",
    "print(data[0][0])\n",
    "print(data[0][1])\n",
    "    \n",
    "np.random.shuffle(data)\n",
    "\n",
    "test_data = np.array(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_model = TransModel(len(data[0][0]), len(data[0][1]))\n",
    "\n",
    "check_model.model.load_state_dict(torch.load('units-transition-model-predict-4.pt'))\n",
    "\n",
    "evaluation(check_model, test_data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
