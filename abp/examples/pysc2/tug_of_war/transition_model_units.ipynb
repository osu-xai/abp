{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from tensorboardX import SummaryWriter\n",
    "import tqdm\n",
    "import os\n",
    "import uuid\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "FloatTensor = torch.cuda.FloatTensor if use_cuda else torch.FloatTensor\n",
    "LongTensor = torch.cuda.LongTensor if use_cuda else torch.LongTensor\n",
    "IntTensor = torch.cuda.IntTensor if use_cuda else torch.IntTensor\n",
    "ByteTensor = torch.cuda.ByteTensor if use_cuda else torch.ByteTensor\n",
    "Tensor = FloatTensor\n",
    "\n",
    "unique_id = str(uuid.uuid4())\n",
    "\n",
    "def weights_initialize(module):\n",
    "    if type(module) == nn.Linear:\n",
    "        nn.init.xavier_uniform_(module.weight, gain=nn.init.calculate_gain('relu'))\n",
    "        module.bias.data.fill_(0.01)\n",
    "        \n",
    "class _TransModel(nn.Module):\n",
    "    \"\"\" Model for DQN \"\"\"\n",
    "\n",
    "    def __init__(self, input_len, output_len):\n",
    "        super(_TransModel, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Sequential(\n",
    "            torch.nn.Linear(input_len, 1024),\n",
    "            torch.nn.BatchNorm1d(1024),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1.apply(weights_initialize)\n",
    "        \n",
    "        self.fc2 = nn.Sequential(\n",
    "            torch.nn.Linear(1024, 512),\n",
    "            torch.nn.BatchNorm1d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc2.apply(weights_initialize)\n",
    "        \n",
    "        self.fc3 = nn.Sequential(\n",
    "            torch.nn.Linear(512, 128),\n",
    "            torch.nn.BatchNorm1d(128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc3.apply(weights_initialize)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            torch.nn.Linear(128, output_len)\n",
    "        )\n",
    "        self.output_layer.apply(weights_initialize)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.fc1(input)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return self.output_layer(x)\n",
    "\n",
    "    \n",
    "class TransModel():\n",
    "    def __init__(self, input_len, ouput_len, learning_rate = 0.0001):\n",
    "        self.model = _TransModel(input_len, ouput_len)\n",
    "        \n",
    "        if use_cuda:\n",
    "            print(\"Using GPU\")\n",
    "            self.model = self.model.cuda()\n",
    "        else:\n",
    "            print(\"Using CPU\")\n",
    "        self.steps = 0\n",
    "        # self.model = nn.DataParallel(self.model)\n",
    "        self.optimizer = Adam(self.model.parameters(), lr = learning_rate)\n",
    "        self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "        \n",
    "        self.steps = 0\n",
    "        \n",
    "    def predict(self, input, steps, learning):\n",
    "        output = self.model(input).squeeze(1)\n",
    "        #reward, next_state = output[0], output[1:]\n",
    "\n",
    "        return output\n",
    "\n",
    "    def predict_batch(self, input):\n",
    "        output = self.model(input)\n",
    "        #reward, next_state = output[:, 0], output[:, 1:]\n",
    "        return output\n",
    "\n",
    "    def fit(self, state, target_state):\n",
    "        loss = self.loss_fn(state, target_state)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.steps += 1\n",
    "        return loss\n",
    "    \n",
    "    def save(self):\n",
    "        cwd = os.getcwd()\n",
    "        path = cwd + '/models'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "        file_path = path + '/UNITS_' + unique_id + '.pt'\n",
    "        torch.save(self.model.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 2) (50000, 2)\n"
     ]
    }
   ],
   "source": [
    "#data = torch.load('test_random_vs_random_2l.pt')\n",
    "data = torch.load('all_experiences_100000.pt')\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "l = len(data)\n",
    "\n",
    "norm_vector = np.array([700, # p1 minerals\n",
    "                        50, 40, 20, 50, 40, 20, 3, # p1 top and bottom lane buildings\n",
    "                        50, 40, 20, 50, 40, 20, 3, # p2 top and bottom lane buildings\n",
    "                        1, 1, 1, 1, 1, 1, # p1 top and bottom lane units\n",
    "                        1, 1, 1, 1, 1, 1, # p1 top and bottom lane units\n",
    "                        2000, 2000, 2000, 2000, 40])\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    #data[i][0] = data[i][0] / norm_vector # normalize input\n",
    "    #data[i][1] = data[i][1][0:32] / norm_vector # normalize ground truth\n",
    "    data[i][0] = data[i][0]\n",
    "    data[i][1] = data[i][1][0:32]\n",
    "\n",
    "    data[i][1] = data[i][1][15:27] # set output to units in top and bottom lanes \n",
    "    \n",
    "#print(data[0][0])\n",
    "#print(data[0][1])\n",
    "\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = np.array(data[: int(np.floor(l * 0.5))])\n",
    "test_data = np.array(data[int(np.floor(l * 0.5)) : ])\n",
    "print(train_data.shape, test_data.shape)\n",
    "\n",
    "batch_size = 64\n",
    "summary_test = SummaryWriter(log_dir = 'units-on-field-transition-model-report/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "0.95622\n"
     ]
    }
   ],
   "source": [
    "baselineCurrent = np.stack(test_data[:, 0])\n",
    "baselineNextUnits = np.stack(test_data[:, 1])\n",
    "baselineCurrentUnits = baselineCurrent[:, 15:27]\n",
    "\n",
    "print(baselineCurrentUnits[0])\n",
    "print(baselineNextUnits[0])\n",
    "\n",
    "mse_baseline = ((baselineCurrentUnits - baselineNextUnits)**2).mean(axis=None)\n",
    "print(mse_baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU\n"
     ]
    }
   ],
   "source": [
    "trans_model = TransModel(len(data[0][0]), len(data[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(model, data, epoch):\n",
    "    state_action = torch.from_numpy(np.stack(data[:, 0])).type(FloatTensor)\n",
    "    next_state_reward = torch.from_numpy(np.stack(data[:, 1])).type(FloatTensor)\n",
    "    \n",
    "    model.model.eval()\n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    outputs = model.predict_batch(state_action)\n",
    "    mse = criterion(outputs, next_state_reward)\n",
    "    mse_p1 = criterion(outputs, next_state_reward)\n",
    "    mse_p2 = criterion(outputs, next_state_reward)\n",
    "\n",
    "    accuracy = torch.sum( torch.sum( torch.eq( outputs, next_state_reward ) )).item()\n",
    "    accuracy = accuracy / (2 * outputs.size()[0])\n",
    "\n",
    "    model.model.train()\n",
    "    \n",
    "    summary_test.add_scalar(\"MSE\", float(mse.item()), epoch)\n",
    "    summary_test.add_scalars(\"MSE\",{'Player 1 Unit MSE': float(mse_p1.item())}, epoch)\n",
    "    summary_test.add_scalars(\"MSE\",{'Player 2 Unit MSE': float(mse_p2.item())}, epoch)\n",
    "    summary_test.add_scalars(\"MSE\",{'Baseline Unit MSE': float(mse_baseline)}, epoch)\n",
    "\n",
    "    #summary_test.add_scalar(tag=\"Accuracy (Correct / Total)\",\n",
    "    #                        scalar_value=float(accuracy),\n",
    "    #                        global_step=epoch)\n",
    "    \n",
    "    f = open(\"units-on-field-transition-model-report/test_loss.txt\", \"a+\")\n",
    "    f.write(\"loss:\" + str(mse.item()) + \", \")\n",
    "    f.write(\"acc:\" + str(accuracy) + \"\\n\")\n",
    "    if epoch % 1000 == 0:\n",
    "        f.write(\"output:\" + str(outputs[0:2]) + \"\\n\")\n",
    "        f.write(\"ground true:\" + str(next_state_reward[0:2]) + \"\\n\")\n",
    "    f.close()\n",
    "    return mse.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/10000 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50000, 32]) torch.Size([50000, 12])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 1/10000 [00:02<5:48:31,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 2/10000 [00:04<5:48:19,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 3/10000 [00:06<5:47:55,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 4/10000 [00:08<5:49:05,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 5/10000 [00:10<5:52:12,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 6/10000 [00:12<5:50:23,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 7/10000 [00:14<5:47:54,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 8/10000 [00:16<5:47:03,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 9/10000 [00:18<5:54:16,  2.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 10/10000 [00:21<5:52:06,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 11/10000 [00:23<5:47:45,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 12/10000 [00:25<5:48:58,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 13/10000 [00:27<5:49:57,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 14/10000 [00:29<5:48:30,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 15/10000 [00:31<5:47:41,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 16/10000 [00:33<5:47:39,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 17/10000 [00:35<5:45:57,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 18/10000 [00:37<5:44:39,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 19/10000 [00:39<5:45:21,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 20/10000 [00:41<5:45:56,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 21/10000 [00:43<5:44:43,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 22/10000 [00:45<5:44:58,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 23/10000 [00:48<5:44:01,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 24/10000 [00:50<5:45:13,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 25/10000 [00:52<5:45:50,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 26/10000 [00:54<5:47:19,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 27/10000 [00:56<5:47:22,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 28/10000 [00:58<5:46:36,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 29/10000 [01:00<5:45:51,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 30/10000 [01:02<5:45:41,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 31/10000 [01:04<5:46:10,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 32/10000 [01:06<5:45:53,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 33/10000 [01:08<5:45:32,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 34/10000 [01:10<5:45:53,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 35/10000 [01:13<5:44:19,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 36/10000 [01:15<5:46:54,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 37/10000 [01:17<5:49:03,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 38/10000 [01:19<5:50:16,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 39/10000 [01:21<5:50:12,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 40/10000 [01:23<5:47:58,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 41/10000 [01:25<5:47:01,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 42/10000 [01:27<5:47:22,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 43/10000 [01:29<5:47:56,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 44/10000 [01:31<5:47:41,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 45/10000 [01:34<5:48:17,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 46/10000 [01:36<5:48:39,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 47/10000 [01:38<5:53:01,  2.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 48/10000 [01:40<5:54:57,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 49/10000 [01:42<5:54:23,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  0%|          | 50/10000 [01:44<5:55:33,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 51/10000 [01:46<5:56:50,  2.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 52/10000 [01:49<5:57:44,  2.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 53/10000 [01:51<5:56:09,  2.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 54/10000 [01:53<5:50:41,  2.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 55/10000 [01:55<5:49:15,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 56/10000 [01:57<5:46:52,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 57/10000 [01:59<5:45:47,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 58/10000 [02:01<5:44:22,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 59/10000 [02:03<5:44:42,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 60/10000 [02:05<5:43:44,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 61/10000 [02:07<5:41:57,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 62/10000 [02:09<5:43:05,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 63/10000 [02:11<5:43:37,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 64/10000 [02:14<5:43:40,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 65/10000 [02:16<5:43:23,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 66/10000 [02:18<5:44:32,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 67/10000 [02:20<5:43:56,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 68/10000 [02:22<5:45:33,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 69/10000 [02:24<5:44:28,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 70/10000 [02:26<5:44:24,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 71/10000 [02:28<5:45:19,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 72/10000 [02:30<5:46:47,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 73/10000 [02:32<5:44:33,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 74/10000 [02:34<5:44:23,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 75/10000 [02:36<5:45:40,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 76/10000 [02:39<5:45:18,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 77/10000 [02:41<5:44:09,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 78/10000 [02:43<5:45:25,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 79/10000 [02:45<5:49:29,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 80/10000 [02:47<5:50:59,  2.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 81/10000 [02:49<5:53:13,  2.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 82/10000 [02:51<5:51:22,  2.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 83/10000 [02:53<5:48:35,  2.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 84/10000 [02:55<5:47:41,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 85/10000 [02:58<5:47:35,  2.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 86/10000 [03:00<5:45:16,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 87/10000 [03:02<5:44:29,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 88/10000 [03:04<5:43:33,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 89/10000 [03:06<5:42:11,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 90/10000 [03:08<5:41:11,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 91/10000 [03:10<5:40:19,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 92/10000 [03:12<5:40:54,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 93/10000 [03:14<5:40:15,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 94/10000 [03:16<5:39:20,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 95/10000 [03:18<5:37:20,  2.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 96/10000 [03:20<5:39:31,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 97/10000 [03:22<5:39:13,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 98/10000 [03:24<5:39:05,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 99/10000 [03:26<5:39:54,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 100/10000 [03:28<5:40:09,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 101/10000 [03:31<5:39:03,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 102/10000 [03:33<5:38:39,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 103/10000 [03:35<5:38:50,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 104/10000 [03:37<5:39:22,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 105/10000 [03:39<5:39:56,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 106/10000 [03:41<5:39:42,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 107/10000 [03:43<5:40:41,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 108/10000 [03:45<5:40:23,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 109/10000 [03:47<5:40:36,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 110/10000 [03:49<5:41:45,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 111/10000 [03:51<5:39:31,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 112/10000 [03:53<5:39:14,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 113/10000 [03:55<5:39:48,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 114/10000 [03:57<5:39:31,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 115/10000 [03:59<5:39:37,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 116/10000 [04:01<5:39:38,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 117/10000 [04:03<5:38:17,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 118/10000 [04:06<5:36:50,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 119/10000 [04:08<5:36:55,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 120/10000 [04:10<5:37:26,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 121/10000 [04:12<5:38:02,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 122/10000 [04:14<5:38:37,  2.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 123/10000 [04:16<5:38:14,  2.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|          | 124/10000 [04:18<5:40:59,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 125/10000 [04:20<5:41:31,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 126/10000 [04:22<5:42:34,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 127/10000 [04:24<5:41:27,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 128/10000 [04:26<5:41:13,  2.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 129/10000 [04:28<5:42:04,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 130/10000 [04:30<5:43:26,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 131/10000 [04:32<5:42:59,  2.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "  1%|▏         | 132/10000 [04:35<5:42:00,  2.08s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "state_action = torch.from_numpy(np.stack(train_data[:, 0])).type(FloatTensor)\n",
    "next_state_reward = torch.from_numpy(np.stack(train_data[:, 1])).type(FloatTensor)\n",
    "print(state_action.size(), next_state_reward.size())\n",
    "\n",
    "for epoch in tqdm.tqdm(range(10000)):\n",
    "    loss = 0\n",
    "    s = np.arange(state_action.shape[0])\n",
    "    np.random.shuffle(s)\n",
    "    train_x = state_action[s]\n",
    "    train_y = next_state_reward[s]\n",
    "    for i in range(state_action.shape[0] // batch_size + 1):\n",
    "        if (i + 1) * batch_size <= state_action.shape[0]:\n",
    "            start = i * batch_size\n",
    "            end = (i + 1) * batch_size\n",
    "        else:\n",
    "            start = i * batch_size\n",
    "            end = state_action.shape[0]\n",
    "        #print(start, end)\n",
    "        inputs, ground_true = train_x[start : end, :], train_y[start : end, :]\n",
    "        outputs = trans_model.predict_batch(inputs)\n",
    "        loss += trans_model.fit(outputs, ground_true)\n",
    "#     print(epoch)\n",
    "    summary_test.add_scalars(\"MSE\",{'Train MSE': float(loss / (state_action.shape[0] // batch_size + 1) )}, epoch)\n",
    "    evaluation(trans_model, test_data, epoch)\n",
    "    #break\n",
    "\n",
    "#trans_model.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
